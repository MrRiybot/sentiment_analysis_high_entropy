{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14734565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing what is important\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import emoji\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import BertModel\n",
    "##nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18c3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model pretrained\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c21ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets with labled sentiment\n",
    "df = pd.read_csv(\"proccesed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba182ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting device \"GPU/CPU\"\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56643c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    '''\n",
    "    text is an arabic string input\n",
    "    \n",
    "    the preprocessed text is returned\n",
    "    '''\n",
    "    \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    \n",
    "    #remove longation\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ad2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data, tokinizer, text_preprocessing_fn = text_preprocessing, MAX_LEN=10):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For every sentence...\n",
    "    for i,sent in enumerate(data):\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing_fn(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            padding='max_length',        # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            truncation = True \n",
    "            )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a91d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(data, tokinizer):\n",
    "    input_id, attention_masks = preprocessing_for_bert(data, tokinizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92b4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50d67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Tweet\n",
    "y = df.Sentiment.replace({\"pos\": 1, \"neg\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab116fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad26d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X.values,y,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33067589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(X_train, tokenizer)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58a0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "188428bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(data, tokinizer):\n",
    "    input_id, attention_masks = preprocessing_for_bert(data, tokinizer)\n",
    "    X_test = torch.tensor(input_ids)\n",
    "    test_data = TensorDataset(X_test, attention_masks)\n",
    "    test_sampler = RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10569a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT :) \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, freeze=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(BertModel, self).__init__()\n",
    "        D_in = 32000 # bert_in ?\n",
    "        H, D_out= 16000,2\n",
    "        \n",
    "        self.bert =  AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-mini-arabic\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fadc8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import SparseAdam, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0123afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \n",
    "    bert_model = BertModel(freeze=False)\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(params=list(bert_model.parameters()), \n",
    "                     lr=5e-5,\n",
    "                     eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    return bert_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4621fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=4):\n",
    "    \n",
    "    print(\"START TRAINING...\")\n",
    "    temp = np.Infinity\n",
    "    val_accuracy = 0\n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss, batch_loss, batch_counts = 0.,0.,0.\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for step,batch in enumerate(train_loader):\n",
    "            if(epoch==0):\n",
    "                continue\n",
    "            batch_counts+=1\n",
    "            inputs_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            logits = model(inputs_ids, attention_mask)\n",
    "            train_loss = criterion(logits,labels)\n",
    "            \n",
    "            batch_loss += train_loss.item()\n",
    "            total_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                val_loss, val_accuracy = evaluate(model, val_loader)\n",
    "                print(\"epoch: {} | step: {} | train_loss: {} | val_loss {} | val_accuracy {} \".format(epoch, step, (batch_loss / batch_counts), val_loss, val_accuracy))\n",
    "                if(val_loss < temp):\n",
    "                    temp = val_loss\n",
    "                    print(\"saving model...\")\n",
    "                    torch.save(model,\"model.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2b7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # For each batch in our validation set...\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6788f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xduck\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc37a6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING...\n",
      "epoch: 1 | step: 100 | train_loss: 0.38263419816399563 | val_loss 0.43375168352479665 | val_accuracy 81.40845070422536 \n",
      "saving model...\n",
      "epoch: 1 | step: 200 | train_loss: 0.35938418916987247 | val_loss 0.5150794959928788 | val_accuracy 80.66901408450704 \n",
      "epoch: 1 | step: 300 | train_loss: 0.35153737629568854 | val_loss 0.4547091201892201 | val_accuracy 80.77464788732394 \n",
      "epoch: 1 | step: 400 | train_loss: 0.3469836261597209 | val_loss 0.4193937877009452 | val_accuracy 82.16549295774648 \n",
      "saving model...\n",
      "epoch: 1 | step: 500 | train_loss: 0.3390265948207911 | val_loss 0.46432274827654935 | val_accuracy 81.91901408450704 \n",
      "epoch: 1 | step: 600 | train_loss: 0.3388026504097088 | val_loss 0.4144747299207768 | val_accuracy 81.90140845070422 \n",
      "saving model...\n",
      "epoch: 1 | step: 700 | train_loss: 0.3332892374883535 | val_loss 0.4721359352095866 | val_accuracy 81.65492957746478 \n",
      "epoch: 1 | step: 800 | train_loss: 0.33256133103954805 | val_loss 0.4327883133271211 | val_accuracy 81.86619718309859 \n",
      "epoch: 1 | step: 900 | train_loss: 0.3320752540468905 | val_loss 0.40968306016334344 | val_accuracy 82.11267605633803 \n",
      "saving model...\n",
      "epoch: 1 | step: 1000 | train_loss: 0.3314748165550289 | val_loss 0.4363736258426183 | val_accuracy 81.21478873239437 \n",
      "epoch: 1 | step: 1100 | train_loss: 0.3283262562808614 | val_loss 0.48624487299944313 | val_accuracy 81.97183098591549 \n",
      "epoch: 1 | step: 1200 | train_loss: 0.32809994707533857 | val_loss 0.407828706943653 | val_accuracy 82.20070422535211 \n",
      "saving model...\n",
      "epoch: 1 | step: 1300 | train_loss: 0.32816143083650273 | val_loss 0.4168568752496175 | val_accuracy 81.84859154929578 \n",
      "epoch: 1 | step: 1400 | train_loss: 0.32759451297783876 | val_loss 0.4055313312251803 | val_accuracy 81.97183098591549 \n",
      "saving model...\n",
      "epoch: 1 | step: 1500 | train_loss: 0.3267989055645081 | val_loss 0.43169182107692033 | val_accuracy 82.04225352112677 \n",
      "epoch: 1 | step: 1600 | train_loss: 0.3270333877123013 | val_loss 0.39905524382918656 | val_accuracy 81.28521126760563 \n",
      "saving model...\n",
      "epoch: 1 | step: 1700 | train_loss: 0.32598529383978164 | val_loss 0.4310863684338163 | val_accuracy 82.1830985915493 \n",
      "epoch: 1 | step: 1800 | train_loss: 0.32541786758916896 | val_loss 0.4272375057071028 | val_accuracy 81.6725352112676 \n",
      "epoch: 1 | step: 1900 | train_loss: 0.32423604822774926 | val_loss 0.4308565362763237 | val_accuracy 82.27112676056338 \n",
      "epoch: 1 | step: 2000 | train_loss: 0.3229154450879402 | val_loss 0.45416836996208615 | val_accuracy 82.48239436619718 \n",
      "epoch: 1 | step: 2100 | train_loss: 0.32214633671071263 | val_loss 0.417723029264262 | val_accuracy 82.65845070422536 \n",
      "epoch: 1 | step: 2200 | train_loss: 0.3207565540272653 | val_loss 0.4118504513422368 | val_accuracy 82.79929577464789 \n",
      "epoch: 1 | step: 2300 | train_loss: 0.3203373087520692 | val_loss 0.3863346988466424 | val_accuracy 82.99295774647888 \n",
      "saving model...\n",
      "epoch: 1 | step: 2400 | train_loss: 0.3192897036319435 | val_loss 0.4462219147929843 | val_accuracy 82.78169014084507 \n",
      "epoch: 1 | step: 2500 | train_loss: 0.31892399358131535 | val_loss 0.3730709725492437 | val_accuracy 82.95774647887323 \n",
      "saving model...\n",
      "epoch: 1 | step: 2600 | train_loss: 0.31791796817192547 | val_loss 0.40716629108073005 | val_accuracy 82.74647887323944 \n",
      "epoch: 1 | step: 2700 | train_loss: 0.31806998926165564 | val_loss 0.3718769520206351 | val_accuracy 83.4330985915493 \n",
      "saving model...\n",
      "epoch: 1 | step: 2800 | train_loss: 0.3182631191265228 | val_loss 0.4228366471512217 | val_accuracy 83.36267605633803 \n",
      "epoch: 1 | step: 2900 | train_loss: 0.31768538277456265 | val_loss 0.40562308648942225 | val_accuracy 81.7605633802817 \n",
      "epoch: 1 | step: 3000 | train_loss: 0.317649740597276 | val_loss 0.4018609540143483 | val_accuracy 82.94014084507042 \n",
      "epoch: 1 | step: 3100 | train_loss: 0.3169992817346558 | val_loss 0.4206765756850511 | val_accuracy 83.27464788732394 \n",
      "epoch: 1 | step: 3194 | train_loss: 0.3166966439780075 | val_loss 0.4449769873644265 | val_accuracy 80.84507042253522 \n",
      "epoch: 2 | step: 100 | train_loss: 0.29006594954298276 | val_loss 0.4834131340325718 | val_accuracy 83.22183098591549 \n",
      "epoch: 2 | step: 200 | train_loss: 0.25045658476113236 | val_loss 0.49088454167555334 | val_accuracy 82.8169014084507 \n",
      "epoch: 2 | step: 300 | train_loss: 0.23486357759003623 | val_loss 0.6285126936103834 | val_accuracy 82.72887323943662 \n",
      "epoch: 2 | step: 400 | train_loss: 0.22604275786636066 | val_loss 0.5394487031328846 | val_accuracy 82.97535211267606 \n",
      "epoch: 2 | step: 500 | train_loss: 0.21960194288016838 | val_loss 0.5762921874660631 | val_accuracy 81.88380281690141 \n",
      "epoch: 2 | step: 600 | train_loss: 0.2202664388831048 | val_loss 0.50589359816636 | val_accuracy 83.16901408450704 \n",
      "epoch: 2 | step: 700 | train_loss: 0.21732981713885938 | val_loss 0.5359228614481611 | val_accuracy 82.78169014084507 \n",
      "epoch: 2 | step: 800 | train_loss: 0.2151577216918963 | val_loss 0.46403090912376493 | val_accuracy 83.04577464788733 \n",
      "epoch: 2 | step: 900 | train_loss: 0.21324501152731246 | val_loss 0.5472489551968978 | val_accuracy 83.3274647887324 \n",
      "epoch: 2 | step: 1000 | train_loss: 0.213963390384744 | val_loss 0.6197882492357576 | val_accuracy 83.20422535211267 \n",
      "epoch: 2 | step: 1100 | train_loss: 0.2139829644200804 | val_loss 0.519347856424644 | val_accuracy 83.15140845070422 \n",
      "epoch: 2 | step: 1200 | train_loss: 0.21355289047699094 | val_loss 0.5579826548302048 | val_accuracy 82.95774647887323 \n",
      "epoch: 2 | step: 1300 | train_loss: 0.21116292817873536 | val_loss 0.6126490749182626 | val_accuracy 82.21830985915493 \n",
      "epoch: 2 | step: 1400 | train_loss: 0.21130284126277954 | val_loss 0.5081534268596852 | val_accuracy 82.85211267605634 \n",
      "epoch: 2 | step: 1500 | train_loss: 0.21174967959130755 | val_loss 0.5729341514670933 | val_accuracy 82.6056338028169 \n",
      "epoch: 2 | step: 1600 | train_loss: 0.21390050366864966 | val_loss 0.5438650370168854 | val_accuracy 82.30633802816901 \n",
      "epoch: 2 | step: 1700 | train_loss: 0.21305484286470308 | val_loss 0.5696564713335583 | val_accuracy 83.11619718309859 \n",
      "epoch: 2 | step: 1800 | train_loss: 0.21333773960034905 | val_loss 0.5708717168007098 | val_accuracy 82.88732394366197 \n",
      "epoch: 2 | step: 1900 | train_loss: 0.2128204173248125 | val_loss 0.621308157993087 | val_accuracy 83.25704225352112 \n",
      "epoch: 2 | step: 2000 | train_loss: 0.21247157164499572 | val_loss 0.5837464634538956 | val_accuracy 83.30985915492958 \n",
      "epoch: 2 | step: 2100 | train_loss: 0.21254373814170158 | val_loss 0.5610926564599217 | val_accuracy 82.88732394366197 \n",
      "epoch: 2 | step: 2200 | train_loss: 0.212196176184643 | val_loss 0.5520964883989327 | val_accuracy 82.62323943661971 \n",
      "epoch: 2 | step: 2300 | train_loss: 0.21173646810916055 | val_loss 0.5438629057546946 | val_accuracy 82.67605633802818 \n",
      "epoch: 2 | step: 2400 | train_loss: 0.21079739898285932 | val_loss 0.6089220877408876 | val_accuracy 82.88732394366197 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23044/427645065.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23044/158923640.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    359\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lr\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787d393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
