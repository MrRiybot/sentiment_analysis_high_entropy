{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrSwi\\anaconda3\\envs\\projects\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pip install torch -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"proccesed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€''' + string.punctuation\n",
    "\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             Ù‘    | # Shadda\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    '''\n",
    "    text is an arabic string input\n",
    "    \n",
    "    the preprocessed text is returned\n",
    "    '''\n",
    "    \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    \n",
    "    #remove longation\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "def preprocessing_for_bert(data, tokinizer, text_preprocessing_fn = text_preprocessing, MAX_LEN=10):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For every sentence...\n",
    "    for i,sent in enumerate(data):\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing_fn(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            padding='max_length',        # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,     # Return attention mask\n",
    "            truncation = True \n",
    "            )\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(data, tokinizer):\n",
    "    input_id, attention_masks = preprocessing_for_bert(data, tokinizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Tweet\n",
    "y = df.Sentiment.replace({\"pos\": 1, \"neg\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Ø§Ù„Ù„Ù‡ Ù…ÙŠØ± ÙŠØ¨Ù‚ÙŠÙƒ ØŒ Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ ÙŠØ§Ø±Ø¨ Ø³Ø§Ø±ÙˆÙ‡ ğŸ’‹\n",
       "1                                    Ù…Ù„ÙƒØ© Ø§Ù„Ø±Ø§Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ğŸ‘‘\n",
       "2                                        Ø¹Ù‚Ø¨Ø§Ù„ ÙƒÙ„ Ù…Ø³Ø±Ø­ÙŠØ© ğŸ˜‚\n",
       "3        Ø§Ø®ÙˆØ§ØªÙŠ ÙˆØ§Ø®ÙˆØ§Ù†ÙŠ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØªØ®Ø§Ù†Ù‚ÙˆØ§ Ù„Ø§ÙŠÙ ÙˆØ¨ÙŠÙ†Ø´Ø±ÙˆØ§ Ø§Ù„...\n",
       "4                                             Ø­Ø§Ø¶Ø± Ø¹Ù…ØªÙŠÙŠ ğŸ˜’\n",
       "                               ...                        \n",
       "56790                 Ø§ØºÙ†ÙŠÙ‡ Ù…ÙƒÙ…Ù„Ù†Ø§Ø´ #ØªØ§Ù…Ø±_Ø¹Ø§Ø´ÙˆØ± ğŸ’” #Ø¨ØµÙˆØªÙŠ ğŸ¶\n",
       "56791    Ø§Ù…Ø³ Ù…Ù† Ø¬Ø¯ Ø¬Ø§Ù‡Ø¯Øª Ù…Ù† Ù‚Ù„Ø¨ Ù„Ø¯Ø±Ø¬Ø© ÙØªØ­Øª Ø§Ù„Ø§Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¸Ù‡...\n",
       "56792    Ø·ÙŠØ¨ Ø§Ø¯Ù„Ø¹ Ø²ÙˆØ¬ØªÙŠ Ø´ÙˆÙŠ ÙÙŠÙ‡Ø§ Ø´ÙŠ Ø§Ø¨Ù„Ù‡ ØºØ§Ø¯Ù‡ . ÙŠØ¹Ù†ÙŠ ØªØ±...\n",
       "56793    Ø£ÙƒØ«Ø± Ø§Ù„Ø£Ù†Ø¯ÙŠØ© Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ© ØªØªÙˆÙŠØ¬Ø§ Ø¨Ù„Ù‚Ø¨ Ø§Ù„Ø¯ÙˆØ±ÙŠ Ø§Ù„Ù…Ø­...\n",
       "56794              Ø¨Ù…ÙˆØª Ù…Ø±Ù‡ Ø­Ù„ÙˆÙ‡ ÙƒÙŠÙ ØªØªÙƒÙ„Ù… ÙƒØ°Ø§ Ù†ÙŠÙ†ÙŠÙŠÙ†ÙŠÙ†ÙŠÙ†ÙŠ\n",
       "Name: Tweet, Length: 56795, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X.values,y,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(X_train, tokenizer)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51115, 10])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51115, 10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51115])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(data, tokinizer):\n",
    "    input_id, attention_masks = preprocessing_for_bert(data, tokinizer)\n",
    "    X_test = torch.tensor(input_ids)\n",
    "    test_data = TensorDataset(X_test, attention_masks)\n",
    "    test_sampler = RandomSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT :) \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, freeze=False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(BertModel, self).__init__()\n",
    "        D_in = 32000 # bert_in ?\n",
    "        H, D_out= 16000,2\n",
    "        \n",
    "        self.bert =  AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-mini-arabic\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert =  AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-base-arabic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import SparseAdam, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \n",
    "    bert_model = BertModel(freeze=False)\n",
    "    \n",
    "    bert_model = bert_model\n",
    "    \n",
    "    optimizer = AdamW(params=list(bert_model.parameters()), \n",
    "                     lr=5e-5,\n",
    "                     eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    \n",
    "    return bert_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=4):\n",
    "    \n",
    "    print(\"START TRAINING...\")\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        total_loss, batch_loss, batch_counts = 0.,0.,0.\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for step,batch in enumerate(train_loader):\n",
    "            batch_counts+=1\n",
    "            \n",
    "            inputs_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            logits = model(inputs_ids, attention_mask)\n",
    "            \n",
    "            train_loss = criterion(logits,labels)\n",
    "            \n",
    "            batch_loss += train_loss.item()\n",
    "            total_loss += train_loss.item()\n",
    "            \n",
    "            train_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            val_losses, val_accuracy = [],[]\n",
    "            \n",
    "            input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "        for batch in val_loader:\n",
    "            val_loss = criterion(logits, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_accuracy = np.mean(val_accuracy)\n",
    "            \n",
    "        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                \n",
    "            print(\"epoch: {} | step: {} | train_loss: {} | validation_loss\".format(epoch, step, (batch_loss / batch_counts), val_loss, val_accuracy))\n",
    "            batch_loss, batch_counts = 0.,0.\n",
    "            \n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING...\n"
     ]
    }
   ],
   "source": [
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
